{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1) Mount Colab."
      ],
      "metadata": {
        "id": "8oNdZOCjjfS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6RbgJPYjfcF",
        "outputId": "ae49b774-f691-400b-fadf-ace318b5d4a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Inspect the thoroughly cleaned train dataset."
      ],
      "metadata": {
        "id": "60FhHZHVjofY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "train_dir = \"/content/drive/MyDrive/Big Data Analysis and Project - a1906525/Datasets/crime_train_final.csv\"\n",
        "df = pd.read_csv(train_dir)\n",
        "\n",
        "# Check datatypes\n",
        "print(df.dtypes)\n",
        "\n",
        "# Count numerical and categorical columns\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "cat_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns\n",
        "\n",
        "print(f\"Numerical columns ({len(num_cols)}): {list(num_cols)}\")\n",
        "print(f\"Categorical columns ({len(cat_cols)}): {list(cat_cols)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CywDC3HGjzJD",
        "outputId": "d822f8d9-87e5-4e3d-f30b-84496bf419ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "highest_offense_description     object\n",
            "highest_offense_code             int64\n",
            "family_violence                 object\n",
            "occurred_date_time_year          int64\n",
            "sine_odt_month                 float64\n",
            "cosine_odt_month               float64\n",
            "sine_odt_week_of_year          float64\n",
            "cosine_odt_week_of_year        float64\n",
            "sine_odt_day                   float64\n",
            "cosine_odt_day                 float64\n",
            "sine_odt_day_of_week           float64\n",
            "cosine_odt_day_of_week         float64\n",
            "sine_odt_hour                  float64\n",
            "cosine_odt_hour                float64\n",
            "sine_odt_minute                float64\n",
            "cosine_odt_minute              float64\n",
            "sine_occurred_time             float64\n",
            "cosine_occurred_time           float64\n",
            "sine_report_time               float64\n",
            "cosine_report_time             float64\n",
            "report_date_time_year            int64\n",
            "sine_rdt_week_of_year          float64\n",
            "cosine_rdt_week_of_year        float64\n",
            "sine_rdt_day                   float64\n",
            "cosine_rdt_day                 float64\n",
            "sine_rdt_hour                  float64\n",
            "cosine_rdt_hour                float64\n",
            "sine_rdt_minute                float64\n",
            "cosine_rdt_minute              float64\n",
            "sine_rdt_month                 float64\n",
            "cosine_rdt_month               float64\n",
            "sine_rdt_day_of_week           float64\n",
            "cosine_rdt_day_of_week         float64\n",
            "location_type                   object\n",
            "apd_sector                      object\n",
            "apd_district                     int64\n",
            "clearance_status                object\n",
            "dtype: object\n",
            "Numerical columns (32): ['highest_offense_code', 'occurred_date_time_year', 'sine_odt_month', 'cosine_odt_month', 'sine_odt_week_of_year', 'cosine_odt_week_of_year', 'sine_odt_day', 'cosine_odt_day', 'sine_odt_day_of_week', 'cosine_odt_day_of_week', 'sine_odt_hour', 'cosine_odt_hour', 'sine_odt_minute', 'cosine_odt_minute', 'sine_occurred_time', 'cosine_occurred_time', 'sine_report_time', 'cosine_report_time', 'report_date_time_year', 'sine_rdt_week_of_year', 'cosine_rdt_week_of_year', 'sine_rdt_day', 'cosine_rdt_day', 'sine_rdt_hour', 'cosine_rdt_hour', 'sine_rdt_minute', 'cosine_rdt_minute', 'sine_rdt_month', 'cosine_rdt_month', 'sine_rdt_day_of_week', 'cosine_rdt_day_of_week', 'apd_district']\n",
            "Categorical columns (5): ['highest_offense_description', 'family_violence', 'location_type', 'apd_sector', 'clearance_status']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Convert the apd_district and highest_offense_code alone to categorical."
      ],
      "metadata": {
        "id": "NMWSFAarj2VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_convert = ['apd_district', 'highest_offense_code']\n",
        "\n",
        "for col in cat_convert:\n",
        "    df[col] = df[col].astype('category')\n",
        "\n",
        "print(df[cat_convert].dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koJ4v0JUj2fw",
        "outputId": "e1c24ec1-e9d6-4886-c68c-e80f82d94a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apd_district            category\n",
            "highest_offense_code    category\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Load and inspect the test dataset."
      ],
      "metadata": {
        "id": "L4M3qge_j7Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "test_dir = \"/content/drive/MyDrive/Big Data Analysis and Project - a1906525/Datasets/crime_test_cleaned.csv\"\n",
        "dft = pd.read_csv(test_dir)\n",
        "\n",
        "# Check datatypes\n",
        "print(dft.dtypes)\n",
        "\n",
        "# Count numerical and categorical columns\n",
        "num_cols = dft.select_dtypes(include=['int64', 'float64']).columns\n",
        "cat_cols = dft.select_dtypes(include=['object', 'category', 'bool']).columns\n",
        "\n",
        "print(f\"Numerical columns ({len(num_cols)}): {list(num_cols)}\")\n",
        "print(f\"Categorical columns ({len(cat_cols)}): {list(cat_cols)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uvYzDxJj7iG",
        "outputId": "8fba8295-4b11-4948-c2f8-d6b4d3c40f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "highest_offense_description     object\n",
            "highest_offense_code             int64\n",
            "family_violence                 object\n",
            "occurred_date_time_year          int64\n",
            "sine_odt_month                 float64\n",
            "cosine_odt_month               float64\n",
            "sine_odt_week_of_year          float64\n",
            "cosine_odt_week_of_year        float64\n",
            "sine_odt_day                   float64\n",
            "cosine_odt_day                 float64\n",
            "sine_odt_day_of_week           float64\n",
            "cosine_odt_day_of_week         float64\n",
            "sine_odt_hour                  float64\n",
            "cosine_odt_hour                float64\n",
            "sine_odt_minute                float64\n",
            "cosine_odt_minute              float64\n",
            "sine_occurred_time             float64\n",
            "cosine_occurred_time           float64\n",
            "sine_report_time               float64\n",
            "cosine_report_time             float64\n",
            "report_date_time_year            int64\n",
            "sine_rdt_week_of_year          float64\n",
            "cosine_rdt_week_of_year        float64\n",
            "sine_rdt_day                   float64\n",
            "cosine_rdt_day                 float64\n",
            "sine_rdt_hour                  float64\n",
            "cosine_rdt_hour                float64\n",
            "sine_rdt_minute                float64\n",
            "cosine_rdt_minute              float64\n",
            "sine_rdt_month                 float64\n",
            "cosine_rdt_month               float64\n",
            "sine_rdt_day_of_week           float64\n",
            "cosine_rdt_day_of_week         float64\n",
            "location_type                   object\n",
            "apd_sector                      object\n",
            "apd_district                     int64\n",
            "clearance_status                object\n",
            "dtype: object\n",
            "Numerical columns (32): ['highest_offense_code', 'occurred_date_time_year', 'sine_odt_month', 'cosine_odt_month', 'sine_odt_week_of_year', 'cosine_odt_week_of_year', 'sine_odt_day', 'cosine_odt_day', 'sine_odt_day_of_week', 'cosine_odt_day_of_week', 'sine_odt_hour', 'cosine_odt_hour', 'sine_odt_minute', 'cosine_odt_minute', 'sine_occurred_time', 'cosine_occurred_time', 'sine_report_time', 'cosine_report_time', 'report_date_time_year', 'sine_rdt_week_of_year', 'cosine_rdt_week_of_year', 'sine_rdt_day', 'cosine_rdt_day', 'sine_rdt_hour', 'cosine_rdt_hour', 'sine_rdt_minute', 'cosine_rdt_minute', 'sine_rdt_month', 'cosine_rdt_month', 'sine_rdt_day_of_week', 'cosine_rdt_day_of_week', 'apd_district']\n",
            "Categorical columns (5): ['highest_offense_description', 'family_violence', 'location_type', 'apd_sector', 'clearance_status']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Convert those columns back to categorical as we face the same problem with the test dataset as well."
      ],
      "metadata": {
        "id": "cAN3DJqGkC6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_convert = ['apd_district', 'highest_offense_code']\n",
        "\n",
        "for col in cat_convert:\n",
        "    dft[col] = dft[col].astype('category')\n",
        "\n",
        "print(dft[cat_convert].dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GXs4t78kDDJ",
        "outputId": "3e8c24a0-8067-48d8-cc02-8c6ca4d2002d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apd_district            category\n",
            "highest_offense_code    category\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Check for NAs in both of them."
      ],
      "metadata": {
        "id": "lMtK7fAekJa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load train\n",
        "train_dir = \"/content/drive/MyDrive/Big Data Analysis and Project - a1906525/Datasets/crime_train_final.csv\"\n",
        "df_train = pd.read_csv(train_dir)\n",
        "\n",
        "# Load test\n",
        "test_dir = \"/content/drive/MyDrive/Big Data Analysis and Project - a1906525/Datasets/crime_test_cleaned.csv\"\n",
        "df_test = pd.read_csv(test_dir)\n",
        "\n",
        "# Check for NAs in train\n",
        "if df_train.isna().sum().sum() == 0:\n",
        "    print(\"No NAs in train dataset.\")\n",
        "else:\n",
        "    print(\"There are missing values in train dataset.\")\n",
        "\n",
        "# Check for NAs in test\n",
        "if df_test.isna().sum().sum() == 0:\n",
        "    print(\"No NAs in test dataset.\")\n",
        "else:\n",
        "    print(\"There are missing values in test dataset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCVOx109kKA6",
        "outputId": "f3810605-9c30-4b9c-c15d-ea0e5b7fdec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No NAs in train dataset.\n",
            "No NAs in test dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Modelling - HistGradientBoosting\n",
        "\n",
        "Note: The balanced accuracy, and recall are lame parameters. So they were not used in the report."
      ],
      "metadata": {
        "id": "BYdYFRvEkPS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76WKkTPBjopj",
        "outputId": "a4bd7517-ca77-4dec-df00-f057b2a5f566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    balanced_accuracy_score, confusion_matrix\n",
        ")\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "# --- Set seed for reproducibility ---\n",
        "SEED = 1906525\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "\n",
        "# --- Paths ---\n",
        "train_path = \"/content/drive/MyDrive/Big Data Analysis and Project - a1906525/Datasets/crime_train_final.csv\"\n",
        "test_path = \"/content/drive/MyDrive/Big Data Analysis and Project - a1906525/Datasets/crime_test_cleaned.csv\"\n",
        "perf_dir = \"/content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Performance Metrics\"\n",
        "cm_dir = \"/content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices\"\n",
        "test_pred_dir = \"/content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results\"\n",
        "for d in [perf_dir, cm_dir, test_pred_dir]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# --- Data load ---\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "cat_cols = [\n",
        "    'highest_offense_description', 'family_violence', 'location_type',\n",
        "    'apd_sector', 'apd_district', 'highest_offense_code'\n",
        "]\n",
        "target = \"clearance_status\"\n",
        "features = [c for c in df_train.columns if c != target]\n",
        "\n",
        "# --- Bucketing high-cardinality categorical columns ---\n",
        "def cap_categories(series, max_cats=250, other_label='Other'):\n",
        "    counts = series.value_counts()\n",
        "    keep = counts.index[:max_cats]\n",
        "    return series.where(series.isin(keep), other_label)\n",
        "\n",
        "for col in ['highest_offense_description', 'highest_offense_code']:\n",
        "    df_train[col] = cap_categories(df_train[col].astype(str), 250)\n",
        "    df_test[col] = cap_categories(df_test[col].astype(str), 250)\n",
        "\n",
        "# Convert all categorical columns (including int/float) to string, then to category dtype for sklearn HGB\n",
        "for c in cat_cols + [target]:\n",
        "    df_train[c] = df_train[c].astype(str).astype('category')\n",
        "    df_test[c] = df_test[c].astype(str).astype('category')\n",
        "\n",
        "# Align category sets across train/test\n",
        "for c in cat_cols + [target]:\n",
        "    all_cats = pd.Index(sorted(set(df_train[c].cat.categories).union(set(df_test[c].cat.categories))))\n",
        "    df_train[c] = df_train[c].cat.set_categories(all_cats)\n",
        "    df_test[c] = df_test[c].cat.set_categories(all_cats)\n",
        "\n",
        "# --- Split train/val ---\n",
        "train, val = train_test_split(\n",
        "    df_train, test_size=0.2, stratify=df_train[target], random_state=SEED\n",
        ")\n",
        "\n",
        "for c in cat_cols + [target]:\n",
        "    train[c] = train[c].astype('category')\n",
        "    val[c] = val[c].astype('category')\n",
        "    train[c] = train[c].cat.set_categories(df_train[c].cat.categories)\n",
        "    val[c] = val[c].cat.set_categories(df_train[c].cat.categories)\n",
        "\n",
        "X_train = train[features].copy().reset_index(drop=True)\n",
        "y_train = train[target].cat.codes.values.ravel()\n",
        "X_val = val[features].copy().reset_index(drop=True)\n",
        "y_val = val[target].cat.codes.values.ravel()\n",
        "X_test = df_test[features].copy().reset_index(drop=True)\n",
        "y_test = df_test[target].cat.codes.values.ravel()\n",
        "\n",
        "# Ensure categorical dtype in X matrices\n",
        "for c in cat_cols:\n",
        "    X_train[c] = X_train[c].astype('category')\n",
        "    X_val[c] = X_val[c].astype('category')\n",
        "    X_test[c] = X_test[c].astype('category')\n",
        "\n",
        "labels = list(train[target].cat.categories)\n",
        "n_classes = len(labels)\n",
        "\n",
        "print(\"\\nSplit sizes:\")\n",
        "print(\"  X_train\", X_train.shape, \"y_train\", y_train.shape)\n",
        "print(\"  X_val\", X_val.shape, \"y_val\", y_val.shape)\n",
        "print(\"  X_test\", X_test.shape, \"y_test\", y_test.shape)\n",
        "\n",
        "# --- Metric functions ---\n",
        "def compute_metrics(y_true, y_pred, labels):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=range(len(labels)))\n",
        "    if cm.shape == (2,2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        spec = tn/(tn+fp) if (tn+fp) else 0\n",
        "        sens = tp/(tp+fn) if (tp+fn) else 0\n",
        "    else:\n",
        "        spec = sens = 0\n",
        "    return cm, spec, sens\n",
        "\n",
        "def get_metrics(y_true, y_pred, probs, labels):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    y_idx = np.arange(len(y_true))\n",
        "    if probs.shape[0] != len(y_true):\n",
        "        probs = probs[:len(y_true), :]\n",
        "    loss = -np.mean(np.log(probs[y_idx, y_true] + 1e-12))\n",
        "    avg = 'binary' if len(labels) == 2 else 'macro'\n",
        "    prec = precision_score(y_true, y_pred, average=avg, zero_division=0)\n",
        "    rec  = recall_score(y_true, y_pred, average=avg, zero_division=0)\n",
        "    f1   = f1_score(y_true, y_pred, average=avg, zero_division=0)\n",
        "    bal  = balanced_accuracy_score(y_true, y_pred)\n",
        "    cm, spec, sens = compute_metrics(y_true, y_pred, labels)\n",
        "    return acc, loss, prec, rec, f1, bal, spec, sens, cm\n",
        "\n",
        "# --- Hyperparameter grid ---\n",
        "iterations_list = [50, 100, 200, 400, 800]       # max_iter\n",
        "depth_list = [3, 5, 7, 10, None]                 # max_depth\n",
        "all_results = []\n",
        "\n",
        "for dep in depth_list:\n",
        "    for ite in iterations_list:\n",
        "        print(f\"\\nRunning: {ite} iterations for depth {dep}\")\n",
        "        model = HistGradientBoostingClassifier(\n",
        "            max_iter=ite,\n",
        "            max_depth=dep,\n",
        "            learning_rate=0.1,\n",
        "            l2_regularization=0.0,\n",
        "            random_state=SEED\n",
        "        )\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        preds_train = model.predict(X_train)\n",
        "        preds_val = model.predict(X_val)\n",
        "        preds_test = model.predict(X_test)\n",
        "        probs_train = model.predict_proba(X_train)\n",
        "        probs_val = model.predict_proba(X_val)\n",
        "        probs_test = model.predict_proba(X_test)\n",
        "\n",
        "        print(\"  preds_train:\", preds_train.shape, \"y_train:\", y_train.shape, \"probs_train:\", probs_train.shape)\n",
        "        print(\"  preds_val:\", preds_val.shape, \"y_val:\", y_val.shape, \"probs_val:\", probs_val.shape)\n",
        "        print(\"  preds_test:\", preds_test.shape, \"y_test:\", y_test.shape, \"probs_test:\", probs_test.shape)\n",
        "\n",
        "        ta, tl, _, _, _, _, _, _, _ = get_metrics(y_train, preds_train, probs_train, labels)\n",
        "        va, vl, _, _, _, _, _, _, _ = get_metrics(y_val, preds_val, probs_val, labels)\n",
        "        test_acc, test_loss, prec, rec, f1, bal_acc, spec, sens, cm = get_metrics(\n",
        "            y_test, preds_test, probs_test, labels)\n",
        "\n",
        "        all_results.append({\n",
        "            'Model': 'HistGradientBoosting', 'depth': dep, 'iterations': ite,\n",
        "            'train_accuracy': ta, 'val_accuracy': va, 'test_accuracy': test_acc,\n",
        "            'train_loss': tl, 'val_loss': vl, 'test_loss': test_loss,\n",
        "            'precision': prec, 'recall': rec, 'f1': f1,\n",
        "            'balanced_accuracy': bal_acc, 'specificity': spec, 'sensitivity': sens\n",
        "        })\n",
        "\n",
        "        # --- Save confusion matrix ---\n",
        "        fname = f\"dep{dep}_ite{ite}\"\n",
        "        cm_path = os.path.join(cm_dir, fname + \".png\")\n",
        "        plt.figure(figsize=(4,4))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=labels, yticklabels=labels)\n",
        "        plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
        "        plt.title(f\"CM â€“ depth={dep}, iterations={ite}\")\n",
        "        plt.tight_layout(); plt.savefig(cm_path); plt.close()\n",
        "\n",
        "        # --- Save random sample of 15 test predictions ---\n",
        "        sample = df_test.copy()\n",
        "        sample['predicted_clearance_status'] = [labels[p] for p in preds_test]\n",
        "        cols = list(sample.columns)\n",
        "        cols.insert(cols.index('clearance_status')+1, cols.pop(cols.index('predicted_clearance_status')))\n",
        "        sample[cols].sample(15).to_csv(\n",
        "            os.path.join(test_pred_dir, fname + \"_predicted.csv\"), index=False\n",
        "        )\n",
        "\n",
        "        print(f\"Train Acc={ta:.4f} | Test Acc={test_acc:.4f}\")\n",
        "        print(f\"Saved CM at {cm_path}, preds at {os.path.join(test_pred_dir, fname + '_predicted.csv')}\")\n",
        "\n",
        "# --- Save summary ---\n",
        "df_all = pd.DataFrame(all_results)\n",
        "summary_path = os.path.join(perf_dir, \"HistGradientBoosting_all_results.xlsx\")\n",
        "df_all.to_excel(summary_path, index=False)\n",
        "print(f\"\\nAll hyperparameter runs complete. Summary at {summary_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIOGbikakYmx",
        "outputId": "e3f5587e-7389-40f5-95f6-98b6791feffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Split sizes:\n",
            "  X_train (93870, 36) y_train (93870,)\n",
            "  X_val (23468, 36) y_val (23468,)\n",
            "  X_test (10426, 36) y_test (10426,)\n",
            "\n",
            "Running: 50 iterations for depth 3\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8831 | Test Acc=0.8603\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep3_ite50.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep3_ite50_predicted.csv\n",
            "\n",
            "Running: 100 iterations for depth 3\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8897 | Test Acc=0.8658\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep3_ite100.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep3_ite100_predicted.csv\n",
            "\n",
            "Running: 200 iterations for depth 3\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8937 | Test Acc=0.8734\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep3_ite200.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep3_ite200_predicted.csv\n",
            "\n",
            "Running: 400 iterations for depth 3\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8937 | Test Acc=0.8734\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep3_ite400.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep3_ite400_predicted.csv\n",
            "\n",
            "Running: 800 iterations for depth 3\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8937 | Test Acc=0.8734\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep3_ite800.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep3_ite800_predicted.csv\n",
            "\n",
            "Running: 50 iterations for depth 5\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8931 | Test Acc=0.8903\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep5_ite50.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep5_ite50_predicted.csv\n",
            "\n",
            "Running: 100 iterations for depth 5\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8986 | Test Acc=0.8917\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep5_ite100.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep5_ite100_predicted.csv\n",
            "\n",
            "Running: 200 iterations for depth 5\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8998 | Test Acc=0.8904\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep5_ite200.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep5_ite200_predicted.csv\n",
            "\n",
            "Running: 400 iterations for depth 5\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8998 | Test Acc=0.8904\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep5_ite400.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep5_ite400_predicted.csv\n",
            "\n",
            "Running: 800 iterations for depth 5\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8998 | Test Acc=0.8904\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep5_ite800.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep5_ite800_predicted.csv\n",
            "\n",
            "Running: 50 iterations for depth 7\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8951 | Test Acc=0.8904\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep7_ite50.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep7_ite50_predicted.csv\n",
            "\n",
            "Running: 100 iterations for depth 7\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.9003 | Test Acc=0.8895\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep7_ite100.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep7_ite100_predicted.csv\n",
            "\n",
            "Running: 200 iterations for depth 7\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.9005 | Test Acc=0.8891\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep7_ite200.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep7_ite200_predicted.csv\n",
            "\n",
            "Running: 400 iterations for depth 7\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.9005 | Test Acc=0.8891\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep7_ite400.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep7_ite400_predicted.csv\n",
            "\n",
            "Running: 800 iterations for depth 7\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.9005 | Test Acc=0.8891\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep7_ite800.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep7_ite800_predicted.csv\n",
            "\n",
            "Running: 50 iterations for depth 10\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8946 | Test Acc=0.8894\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep10_ite50.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep10_ite50_predicted.csv\n",
            "\n",
            "Running: 100 iterations for depth 10\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.9002 | Test Acc=0.8907\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep10_ite100.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep10_ite100_predicted.csv\n",
            "\n",
            "Running: 200 iterations for depth 10\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.9002 | Test Acc=0.8907\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep10_ite200.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep10_ite200_predicted.csv\n",
            "\n",
            "Running: 400 iterations for depth 10\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.9002 | Test Acc=0.8907\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep10_ite400.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep10_ite400_predicted.csv\n",
            "\n",
            "Running: 800 iterations for depth 10\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.9002 | Test Acc=0.8907\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/dep10_ite800.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/dep10_ite800_predicted.csv\n",
            "\n",
            "Running: 50 iterations for depth None\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8949 | Test Acc=0.8895\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/depNone_ite50.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/depNone_ite50_predicted.csv\n",
            "\n",
            "Running: 100 iterations for depth None\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8998 | Test Acc=0.8905\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/depNone_ite100.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/depNone_ite100_predicted.csv\n",
            "\n",
            "Running: 200 iterations for depth None\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8998 | Test Acc=0.8905\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/depNone_ite200.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/depNone_ite200_predicted.csv\n",
            "\n",
            "Running: 400 iterations for depth None\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8998 | Test Acc=0.8905\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/depNone_ite400.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/depNone_ite400_predicted.csv\n",
            "\n",
            "Running: 800 iterations for depth None\n",
            "  preds_train: (93870,) y_train: (93870,) probs_train: (93870, 2)\n",
            "  preds_val: (23468,) y_val: (23468,) probs_val: (23468, 2)\n",
            "  preds_test: (10426,) y_test: (10426,) probs_test: (10426, 2)\n",
            "Train Acc=0.8998 | Test Acc=0.8905\n",
            "Saved CM at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Confusion Matrices/depNone_ite800.png, preds at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Test Results/depNone_ite800_predicted.csv\n",
            "\n",
            "All hyperparameter runs complete. Summary at /content/drive/MyDrive/Big Data Analysis and Project - a1906525/Results/HistGradientBoosting/Performance Metrics/HistGradientBoosting_all_results.xlsx\n"
          ]
        }
      ]
    }
  ]
}